{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.083468    0.8461055   2.01481512 -0.86019954  0.76324429  0.14131644\n",
      "  -0.06803275 -0.56301422 -0.7459156   0.44127388  1.22573458  0.05915903\n",
      "  -0.36997219  1.50206551 -0.41513822 -0.61791897  1.32205938  0.22123426\n",
      "  -0.3819525   1.51818716]\n",
      " [ 0.32844007  1.35775848  1.293389    1.12271049 -0.64875048 -1.69902919\n",
      "  -0.82641031  0.53365654 -1.23708619  0.46984492  2.22391763  1.27909974\n",
      "   0.89938878 -0.37142159 -0.17455635  0.68603107 -1.27673062  0.48475429\n",
      "   1.03214067 -0.13593122]\n",
      " [-0.00996292  1.03772716  0.04038206 -0.40296852  0.87724177  1.0635165\n",
      "   1.69528737 -0.16215874  1.37864492 -0.09715551 -0.11198136  0.05049977\n",
      "  -0.00935499 -0.5742411   0.11337189 -0.82879811  0.4808113  -2.27411008\n",
      "   1.48903458  0.41312729]\n",
      " [-1.61060529  0.31383363  2.12023621 -1.67273885  0.37760096 -0.9285172\n",
      "   0.93782154  0.26445134 -0.67529663  0.37507568  0.97961717 -0.20547234\n",
      "   0.08551471  0.1461559   0.18198137 -0.41214544  0.56940285  2.01796693\n",
      "   1.77261447 -0.89202825]\n",
      " [-1.12304332 -0.67051374 -0.75075873 -0.78599519  0.04366473 -0.44358682\n",
      "  -1.53559085  0.55338327 -0.60803401  0.6436419   0.24204224  0.09746719\n",
      "  -0.26049799 -0.3210562   0.99182769 -0.134578    0.03805268 -0.87025597\n",
      "   0.45060808 -0.15980811]\n",
      " [-0.45140638  0.79703906  0.3002318   1.03558246  1.2257317  -0.24382803\n",
      "  -0.09178691 -1.22285492  0.2193018   0.19708558 -0.84350716 -0.70198154\n",
      "  -0.47512823 -0.09432866 -0.31548009  0.33454573 -1.54333753 -1.01680287\n",
      "  -0.08162299 -0.96177886]\n",
      " [-0.38915971  0.46427966 -1.28680382 -1.45551333  0.56216646 -0.4930835\n",
      "  -1.16380789  0.17586825 -1.6128473   1.88349487  1.06505756 -0.30221525\n",
      "   0.10326836  0.48425244 -1.27240708  0.76040752 -0.995118   -0.61788184\n",
      "   0.39678326 -1.06123364]\n",
      " [-0.41045064 -0.08496257  0.51142276 -0.68848456  0.34387767  0.51812681\n",
      "  -0.57367307 -0.27244353 -0.14369908  0.95914469 -0.29967246  1.42779539\n",
      "  -0.05799961 -0.86721165 -1.80458899  1.65100026  0.43954197  2.50392643\n",
      "   0.07832857 -1.25907093]\n",
      " [ 1.75256954 -0.43333327 -1.25878605 -1.06487688  0.48574942 -0.72066073\n",
      "   0.54341215 -0.84290516  1.1341428   0.24940529  0.36721727  0.7868904\n",
      "  -1.13209859  0.69193972  1.03725416  0.90109425  0.01965575  0.58277623\n",
      "  -1.37083653  0.22972671]\n",
      " [ 0.04431291 -0.23704609  0.43193285  0.42843158  0.07289453  0.14976835\n",
      "  -0.41838834  0.9234542   0.23969617 -0.19312784 -1.3497318  -0.33333508\n",
      "   0.33854984  1.38758126  0.02633894  0.47782447  2.03649991 -0.8733819\n",
      "   0.3063835  -0.53196151]]\n"
     ]
    }
   ],
   "source": [
    "numpy_tensor = np.random.randn(10,20)\n",
    "print(numpy_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0835,  0.8461,  2.0148, -0.8602,  0.7632,  0.1413, -0.0680, -0.5630,\n",
      "         -0.7459,  0.4413,  1.2257,  0.0592, -0.3700,  1.5021, -0.4151, -0.6179,\n",
      "          1.3221,  0.2212, -0.3820,  1.5182],\n",
      "        [ 0.3284,  1.3578,  1.2934,  1.1227, -0.6488, -1.6990, -0.8264,  0.5337,\n",
      "         -1.2371,  0.4698,  2.2239,  1.2791,  0.8994, -0.3714, -0.1746,  0.6860,\n",
      "         -1.2767,  0.4848,  1.0321, -0.1359],\n",
      "        [-0.0100,  1.0377,  0.0404, -0.4030,  0.8772,  1.0635,  1.6953, -0.1622,\n",
      "          1.3786, -0.0972, -0.1120,  0.0505, -0.0094, -0.5742,  0.1134, -0.8288,\n",
      "          0.4808, -2.2741,  1.4890,  0.4131],\n",
      "        [-1.6106,  0.3138,  2.1202, -1.6727,  0.3776, -0.9285,  0.9378,  0.2645,\n",
      "         -0.6753,  0.3751,  0.9796, -0.2055,  0.0855,  0.1462,  0.1820, -0.4121,\n",
      "          0.5694,  2.0180,  1.7726, -0.8920],\n",
      "        [-1.1230, -0.6705, -0.7508, -0.7860,  0.0437, -0.4436, -1.5356,  0.5534,\n",
      "         -0.6080,  0.6436,  0.2420,  0.0975, -0.2605, -0.3211,  0.9918, -0.1346,\n",
      "          0.0381, -0.8703,  0.4506, -0.1598],\n",
      "        [-0.4514,  0.7970,  0.3002,  1.0356,  1.2257, -0.2438, -0.0918, -1.2229,\n",
      "          0.2193,  0.1971, -0.8435, -0.7020, -0.4751, -0.0943, -0.3155,  0.3345,\n",
      "         -1.5433, -1.0168, -0.0816, -0.9618],\n",
      "        [-0.3892,  0.4643, -1.2868, -1.4555,  0.5622, -0.4931, -1.1638,  0.1759,\n",
      "         -1.6128,  1.8835,  1.0651, -0.3022,  0.1033,  0.4843, -1.2724,  0.7604,\n",
      "         -0.9951, -0.6179,  0.3968, -1.0612],\n",
      "        [-0.4105, -0.0850,  0.5114, -0.6885,  0.3439,  0.5181, -0.5737, -0.2724,\n",
      "         -0.1437,  0.9591, -0.2997,  1.4278, -0.0580, -0.8672, -1.8046,  1.6510,\n",
      "          0.4395,  2.5039,  0.0783, -1.2591],\n",
      "        [ 1.7526, -0.4333, -1.2588, -1.0649,  0.4857, -0.7207,  0.5434, -0.8429,\n",
      "          1.1341,  0.2494,  0.3672,  0.7869, -1.1321,  0.6919,  1.0373,  0.9011,\n",
      "          0.0197,  0.5828, -1.3708,  0.2297],\n",
      "        [ 0.0443, -0.2370,  0.4319,  0.4284,  0.0729,  0.1498, -0.4184,  0.9235,\n",
      "          0.2397, -0.1931, -1.3497, -0.3333,  0.3385,  1.3876,  0.0263,  0.4778,\n",
      "          2.0365, -0.8734,  0.3064, -0.5320]])\n",
      "tensor([[-0.0835,  0.8461,  2.0148, -0.8602,  0.7632,  0.1413, -0.0680, -0.5630,\n",
      "         -0.7459,  0.4413,  1.2257,  0.0592, -0.3700,  1.5021, -0.4151, -0.6179,\n",
      "          1.3221,  0.2212, -0.3820,  1.5182],\n",
      "        [ 0.3284,  1.3578,  1.2934,  1.1227, -0.6488, -1.6990, -0.8264,  0.5337,\n",
      "         -1.2371,  0.4698,  2.2239,  1.2791,  0.8994, -0.3714, -0.1746,  0.6860,\n",
      "         -1.2767,  0.4848,  1.0321, -0.1359],\n",
      "        [-0.0100,  1.0377,  0.0404, -0.4030,  0.8772,  1.0635,  1.6953, -0.1622,\n",
      "          1.3786, -0.0972, -0.1120,  0.0505, -0.0094, -0.5742,  0.1134, -0.8288,\n",
      "          0.4808, -2.2741,  1.4890,  0.4131],\n",
      "        [-1.6106,  0.3138,  2.1202, -1.6727,  0.3776, -0.9285,  0.9378,  0.2645,\n",
      "         -0.6753,  0.3751,  0.9796, -0.2055,  0.0855,  0.1462,  0.1820, -0.4121,\n",
      "          0.5694,  2.0180,  1.7726, -0.8920],\n",
      "        [-1.1230, -0.6705, -0.7508, -0.7860,  0.0437, -0.4436, -1.5356,  0.5534,\n",
      "         -0.6080,  0.6436,  0.2420,  0.0975, -0.2605, -0.3211,  0.9918, -0.1346,\n",
      "          0.0381, -0.8703,  0.4506, -0.1598],\n",
      "        [-0.4514,  0.7970,  0.3002,  1.0356,  1.2257, -0.2438, -0.0918, -1.2229,\n",
      "          0.2193,  0.1971, -0.8435, -0.7020, -0.4751, -0.0943, -0.3155,  0.3345,\n",
      "         -1.5433, -1.0168, -0.0816, -0.9618],\n",
      "        [-0.3892,  0.4643, -1.2868, -1.4555,  0.5622, -0.4931, -1.1638,  0.1759,\n",
      "         -1.6128,  1.8835,  1.0651, -0.3022,  0.1033,  0.4843, -1.2724,  0.7604,\n",
      "         -0.9951, -0.6179,  0.3968, -1.0612],\n",
      "        [-0.4105, -0.0850,  0.5114, -0.6885,  0.3439,  0.5181, -0.5737, -0.2724,\n",
      "         -0.1437,  0.9591, -0.2997,  1.4278, -0.0580, -0.8672, -1.8046,  1.6510,\n",
      "          0.4395,  2.5039,  0.0783, -1.2591],\n",
      "        [ 1.7526, -0.4333, -1.2588, -1.0649,  0.4857, -0.7207,  0.5434, -0.8429,\n",
      "          1.1341,  0.2494,  0.3672,  0.7869, -1.1321,  0.6919,  1.0373,  0.9011,\n",
      "          0.0197,  0.5828, -1.3708,  0.2297],\n",
      "        [ 0.0443, -0.2370,  0.4319,  0.4284,  0.0729,  0.1498, -0.4184,  0.9235,\n",
      "          0.2397, -0.1931, -1.3497, -0.3333,  0.3385,  1.3876,  0.0263,  0.4778,\n",
      "          2.0365, -0.8734,  0.3064, -0.5320]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "pytorch_tensor1 = torch.Tensor(numpy_tensor)\n",
    "pytorch_tensor2 = torch.from_numpy(numpy_tensor)\n",
    "print(pytorch_tensor1)\n",
    "print(pytorch_tensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_array = pytorch_tensor1.numpy()\n",
    "#numpy_array = pytorch_tensor1.cpu().numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Cannot initialize CUDA without ATen_cuda library. PyTorch splits its backend into two shared libraries: a CPU library and a CUDA library; this error has occurred because you are trying to use some CUDA functionality, but the CUDA library has not been loaded by the dynamic linker for some reason.  The CUDA library MUST be loaded, EVEN IF you don't directly use any symbols from the CUDA library! One common culprit is a lack of -Wl,--no-as-needed in your link arguments; many dynamic linkers will delete dynamic library dependencies if you don't depend on any of their symbols.  You can check if this has occurred by using ldd on your binary to see if there is a dependency on *_cuda.so library.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-863ccacb760a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mgpu_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mgpu_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mgpu_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Cannot initialize CUDA without ATen_cuda library. PyTorch splits its backend into two shared libraries: a CPU library and a CUDA library; this error has occurred because you are trying to use some CUDA functionality, but the CUDA library has not been loaded by the dynamic linker for some reason.  The CUDA library MUST be loaded, EVEN IF you don't directly use any symbols from the CUDA library! One common culprit is a lack of -Wl,--no-as-needed in your link arguments; many dynamic linkers will delete dynamic library dependencies if you don't depend on any of their symbols.  You can check if this has occurred by using ldd on your binary to see if there is a dependency on *_cuda.so library."
     ]
    }
   ],
   "source": [
    "dtype = torch.cuda.FloatTensor\n",
    "gpu_tensor = torch.randn(10,20).type(dtype)\n",
    "\n",
    "gpu_tensor = torch.randn(10,20).cuda(0)\n",
    "gpu_tensor = torch.randn(10,20).cuda(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gpu_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-618456b7e413>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcpu_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgpu_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'gpu_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "cpu_tensor = gpu_tensor.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20])\n",
      "torch.Size([10, 20])\n",
      "torch.FloatTensor\n",
      "2\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "print(pytorch_tensor1.shape)\n",
    "print(pytorch_tensor1.size())\n",
    "print(pytorch_tensor1.type())\n",
    "print(pytorch_tensor1.dim())\n",
    "print(pytorch_tensor1.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "extensor = torch.randn(3,2).type(torch.float64)\n",
    "exnum = extensor.numpy()\n",
    "print(exnum.size)\n",
    "print(exnum.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "torch.FloatTensor\n",
      "tensor([[1, 1],\n",
      "        [1, 1]])\n",
      "torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2,2)\n",
    "print(x)\n",
    "print(x.type())\n",
    "x = x.long()\n",
    "print(x)\n",
    "print(x.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1498, -0.9257,  0.4755],\n",
      "        [ 0.6373, -0.6279, -0.0356],\n",
      "        [ 0.1847, -1.4123, -1.8298],\n",
      "        [ 0.1446,  0.2520,  0.2084]])\n",
      "tensor([0.4755, 0.6373, 0.1847, 0.2520])\n",
      "tensor([2, 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4,3)\n",
    "print(x)\n",
    "max_value, max_idx = torch.max(x,dim=1)\n",
    "print(max_value)\n",
    "print(max_idx)\n",
    "sum_x = torch.sum(x, dim=1)\n",
    "print(sum_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n",
      "torch.Size([1, 4, 3])\n",
      "torch.Size([1, 1, 4, 3])\n",
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "x = x.unsqueeze(0)\n",
    "print(x.shape)\n",
    "\n",
    "x = x.unsqueeze(1)\n",
    "print(x.shape)\n",
    "\n",
    "x = x.squeeze()\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 5])\n",
      "tensor([[[-0.3462,  2.8270,  0.3841,  0.4083, -0.2538],\n",
      "         [ 0.3054, -0.4435,  0.0253,  0.8146, -0.3237],\n",
      "         [-0.1145, -2.2570,  0.9820, -1.1142,  0.7558],\n",
      "         [-0.9290,  0.2865, -2.3827, -1.5568, -1.5452]],\n",
      "\n",
      "        [[-0.3760,  0.1415, -1.5967,  1.2326,  0.2184],\n",
      "         [-0.3739,  2.0240, -0.8485,  1.4438,  1.4150],\n",
      "         [-0.3809,  1.4008, -0.5829,  1.7134, -0.6310],\n",
      "         [ 0.0556, -2.1446, -1.2157, -0.2588, -0.9825]],\n",
      "\n",
      "        [[ 0.2802,  0.0282, -0.9078, -0.4914, -0.3226],\n",
      "         [-1.3922, -0.1659, -1.1172,  0.1645,  0.0420],\n",
      "         [-0.3107, -1.1054,  1.8818,  1.1796, -0.5975],\n",
      "         [ 1.1332, -1.3330,  1.5681, -0.7381, -1.5442]]])\n",
      "torch.Size([4, 3, 5])\n",
      "tensor([[[-0.3462,  2.8270,  0.3841,  0.4083, -0.2538],\n",
      "         [-0.3760,  0.1415, -1.5967,  1.2326,  0.2184],\n",
      "         [ 0.2802,  0.0282, -0.9078, -0.4914, -0.3226]],\n",
      "\n",
      "        [[ 0.3054, -0.4435,  0.0253,  0.8146, -0.3237],\n",
      "         [-0.3739,  2.0240, -0.8485,  1.4438,  1.4150],\n",
      "         [-1.3922, -0.1659, -1.1172,  0.1645,  0.0420]],\n",
      "\n",
      "        [[-0.1145, -2.2570,  0.9820, -1.1142,  0.7558],\n",
      "         [-0.3809,  1.4008, -0.5829,  1.7134, -0.6310],\n",
      "         [-0.3107, -1.1054,  1.8818,  1.1796, -0.5975]],\n",
      "\n",
      "        [[-0.9290,  0.2865, -2.3827, -1.5568, -1.5452],\n",
      "         [ 0.0556, -2.1446, -1.2157, -0.2588, -0.9825],\n",
      "         [ 1.1332, -1.3330,  1.5681, -0.7381, -1.5442]]])\n",
      "torch.Size([5, 3, 4])\n",
      "tensor([[[-0.3462,  0.3054, -0.1145, -0.9290],\n",
      "         [-0.3760, -0.3739, -0.3809,  0.0556],\n",
      "         [ 0.2802, -1.3922, -0.3107,  1.1332]],\n",
      "\n",
      "        [[ 2.8270, -0.4435, -2.2570,  0.2865],\n",
      "         [ 0.1415,  2.0240,  1.4008, -2.1446],\n",
      "         [ 0.0282, -0.1659, -1.1054, -1.3330]],\n",
      "\n",
      "        [[ 0.3841,  0.0253,  0.9820, -2.3827],\n",
      "         [-1.5967, -0.8485, -0.5829, -1.2157],\n",
      "         [-0.9078, -1.1172,  1.8818,  1.5681]],\n",
      "\n",
      "        [[ 0.4083,  0.8146, -1.1142, -1.5568],\n",
      "         [ 1.2326,  1.4438,  1.7134, -0.2588],\n",
      "         [-0.4914,  0.1645,  1.1796, -0.7381]],\n",
      "\n",
      "        [[-0.2538, -0.3237,  0.7558, -1.5452],\n",
      "         [ 0.2184,  1.4150, -0.6310, -0.9825],\n",
      "         [-0.3226,  0.0420, -0.5975, -1.5442]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,4,5)\n",
    "print(x.shape)\n",
    "print(x)\n",
    "\n",
    "x = x.permute(1,0,2)\n",
    "print(x.shape)\n",
    "print(x)\n",
    "\n",
    "x = x.transpose(0,2)\n",
    "print(x.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 5])\n",
      "torch.Size([12, 5])\n",
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,4,5)\n",
    "print(x.shape)\n",
    "\n",
    "x = x.view(-1,5)\n",
    "print(x.shape)\n",
    "\n",
    "x = x.view(3,20)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0373,  1.6782,  0.5450,  1.3289],\n",
      "        [ 1.5488, -0.5806, -1.9223, -1.8061],\n",
      "        [ 1.1816, -0.3267, -1.0400,  0.9854]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,4)\n",
    "y = torch.randn(3,4)\n",
    "z = x + y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 3])\n",
      "torch.Size([3, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(3,3)\n",
    "x.unsqueeze_(0)\n",
    "print(x.shape)\n",
    "x.transpose_(1,0)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(3,3)\n",
    "y = torch.ones(3,3)\n",
    "\n",
    "print(x)\n",
    "x.add_(y)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 2., 2., 1.],\n",
      "        [1., 2., 2., 1.],\n",
      "        [1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(4,4).float()\n",
    "print(x)\n",
    "x[1:3,1:3] = 2\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
